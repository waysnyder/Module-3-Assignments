{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 07: Ensemble Methods – Gradient Boosting\n",
    "\n",
    "Over the past two weeks, we have expanded our machine learning toolkit by moving beyond linear regression to decision trees, which require careful selection of interacting hyperparameters. In Homework 6, you developed a systematic workflow for parameter tuning that balanced manual exploration with automated search methods (e.g., grid search) in order to optimize performance while gaining insight into model behavior.\n",
    "\n",
    "This week, we take another step forward by studying **ensemble methods**, which combine multiple decision trees to produce stronger predictive models. Specifically, we will investigate **Gradient Boosting**, one of the most powerful and widely used non-deep learning methods in modern machine learning practice.\n",
    "\n",
    "### What We Will Do in This Homework\n",
    "\n",
    "To analyze and optimize our ensemble models, we will apply the two-phase strategy introduced in Homework 6. However, we will add **two new tools** to our tuning workflow:\n",
    "\n",
    "* We will employ **randomized search instead of exhaustive grid search** to efficiently explore the hyperparameter space and identify promising regions without incurring the full combinatorial cost of evaluating every possible configuration.\n",
    "* You will **store your best parameter values (and the resulting CV MAE) in a dictionary** to track improvements across experiments and maintain a clear record of how each parameter choice was made. This disciplined bookkeeping is essential for systematic model tuning.\n",
    "\n",
    "Our two-phase strategy is thus (with new features in italics):\n",
    "\n",
    "#### 1. First Phase\n",
    "\n",
    "* Iteratively sweep through key parameters in **coarse ranges**, recording results in a dictionary\n",
    "* Visualize training and cross-validation MAE\n",
    "* Diagnose overfitting or underfitting\n",
    "* Repeat the sweeps with progressively finer granularity\n",
    "  (e.g., for `n_estimators`, you might begin with steps of 100, then refine to steps of 50, then 25, etc.)\n",
    "\n",
    "#### 2. Second Phase\n",
    "\n",
    "* Focus on the most promising or unstable parameter ranges identified in Phase 1\n",
    "* Perform a randomized search within these narrower ranges using `RandomizedSearchCV`\n",
    "\n",
    "We will follow this process for `GradientBoostingRegressor`, systematically tuning the most important parameters (see **Appendix 1** for a complete list):\n",
    "\n",
    "`n_estimators`, `max_depth`, `max_features`, `min_samples_split`, `min_samples_leaf`\n",
    "\n",
    "### Why We Are Not Tuning `learning_rate`\n",
    "\n",
    "We will **not** tune `learning_rate` in this assignment.\n",
    "\n",
    "In practice, `learning_rate` and `n_estimators` are tightly coupled: reducing the learning rate typically requires increasing the number of boosting stages. However, to isolate and reinforce the workflow of structured hyperparameter tuning, we will hold `learning_rate` fixed at its default value.\n",
    "\n",
    "Gradient boosting models can appear to improve continuously as more estimators are added (particularly when the learning rate is small). However, this can lead to subtle overfitting. Overfitting can be controlled in several ways, including:\n",
    "\n",
    "* Early stopping\n",
    "* Monitoring the variance of cross-validation scores\n",
    "* Comparing with a held-out test set (as a final check only)\n",
    "\n",
    "In this homework, we will control overfitting in a simplified way by fixing the learning rate and tuning the remaining structural parameters. The goal is to strengthen your understanding of systematic model selection.\n",
    "\n",
    "### Cross-Validation Strategy\n",
    "\n",
    "We will continue using `RepeatedKFold` cross-validation to reduce variance in our CV MAE estimates.\n",
    "\n",
    "However, computational cost increases quickly with the number of repeats. Use a staged approach:\n",
    "\n",
    "* **Explore:** `n_repeats = 1`\n",
    "* **Confirm:** `n_repeats = 3`\n",
    "* **Finalize:** `n_repeats = 5` (only on your final 1–2 candidate models)\n",
    "\n",
    "Be strategic about computational budget. Large sweeps with many repeats can significantly increase runtime.\n",
    "\n",
    "### Before Starting\n",
    "\n",
    "* Review the ensemble methods lesson materials, especially the Gradient Boosting video and notebook. The code in this homework builds directly on those resources.\n",
    "* Refer to **Appendix 2** for tuning strategy guidance.\n",
    "* Refer to **Appendix 3** for a comparison of randomized search and exhaustive grid search.\n",
    "\n",
    "### Grading\n",
    "\n",
    "This homework consists of 4 graded problems, each worth 13 points, and you get 3 points free if you complete the homework. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import time\n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble        import GradientBoostingRegressor\n",
    "from sklearn.metrics         import mean_absolute_error\n",
    "from tqdm                    import tqdm\n",
    "\n",
    "import matplotlib.ticker as mticker           # Optional: you can print out y axis labels as dollars. \n",
    "\n",
    "# globals\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "# utility code\n",
    "\n",
    "# Optional:  Format y-axis labels as dollars with commas\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Ames Housing Dataset  \n",
    "\n",
    "The code cell below will load the dataset for you.  This is the same dataset we used for the last two homeworks. \n",
    "\n",
    "> **Notice** that this code includes a useful optimization: **before downloading, it first\n",
    "checks whether the files already exist.** This is an essential step when working with large datasets or when building deep learning models, where training can span hours or even days. By reusing previously downloaded files or saved models, you can avoid unnecessary work and significantly speed up your workflow.\n",
    "\n",
    "For a detailed description of the dataset features, please refer to the **Appendix** in Homework 05. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset files already exist. Skipping download.\n",
      "Training and testing datasets loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"Ames_Dataset\"                              # Directory where files will be stored\n",
    "\n",
    "# Check if one of the files exists; if not, download and extract the zip file\n",
    "\n",
    "if not os.path.exists( os.path.join(data_dir, \"X_train.csv\") ):\n",
    "    print(\"Dataset files not found. Downloading...\")\n",
    "    zip_url = \"https://www.cs.bu.edu/fac/snyder/cs505/Data/ames_housing.zip\"\n",
    "    try:\n",
    "        response = requests.get(zip_url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        # Extract the zip file into the designated directory\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as zipf:\n",
    "            zipf.extractall(data_dir)\n",
    "        print(\"Files downloaded and extracted successfully.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading the file: {e}\")\n",
    "else:\n",
    "    print(\"Dataset files already exist. Skipping download.\")\n",
    "\n",
    "# Load the datasets\n",
    "X_train = pd.read_csv(os.path.join(data_dir, \"X_train.csv\"))\n",
    "X_test  = pd.read_csv(os.path.join(data_dir, \"X_test.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(data_dir, \"y_train.csv\")).squeeze(\"columns\")    \n",
    "y_test  = pd.read_csv(os.path.join(data_dir, \"y_test.csv\")).squeeze(\"columns\")\n",
    "\n",
    "print(\"Training and testing datasets loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelude: Wrapper Functions for Running Ensemble Models\n",
    "\n",
    "The following cells are adapted from the Week 7 video notebook on `GradientBoostingRegressor`, but have been refactored to be more generally useful (perhaps in your final project):\n",
    "\n",
    "- **`run_model`** replaces the original `run_gradient_boosting_regressor` and accepts a parameter dictionary that can be applied to any model. You do not need to call this explicitly in this homework. \n",
    "- **`sweep_parameter`** is updated to work seamlessly with `run_model`, letting you:\n",
    "  - Specify which model you want to use;  \n",
    "  - Pass a dictionary of model parameters; and  \n",
    "  - Return a modified parameter dictionary reflecting the best value of the parameter you swept, along with the corresponding MAE.\n",
    "\n",
    "**Note:** Please do not change these cells unless you consult with the LFs first. Any alterations may cause downstream issues with the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model for testing and returning metrics.\n",
    "# NOTE: You can NOT use this for running the model on the test set.\n",
    "\n",
    "def run_model(model, \n",
    "              X_train, y_train, \n",
    "              n_repeats=5, \n",
    "              n_jobs=-1, \n",
    "              **model_params\n",
    "             ):\n",
    "\n",
    "    # Instantiate the model if a class is provided\n",
    "    if isinstance(model, type):\n",
    "        model = model(**model_params)\n",
    "    else:                                    \n",
    "        model.set_params(**model_params)    \n",
    "\n",
    "    # Use negative MAE for cross-validation (since sklearn minimizes loss)\n",
    "    neg_mae_scores = cross_val_score(\n",
    "        model, \n",
    "        X_train, y_train,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=RepeatedKFold(n_splits=5, n_repeats=n_repeats, random_state=random_seed), \n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    \n",
    "    mean_cv_mae = -np.mean(neg_mae_scores)\n",
    "    std_cv_mae  =  np.std(neg_mae_scores)\n",
    "    \n",
    "    # Fit the model on the full training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute training MAE\n",
    "    train_preds = model.predict(X_train)\n",
    "    train_mae   = mean_absolute_error(y_train, train_preds)\n",
    "    \n",
    "    return mean_cv_mae, std_cv_mae, train_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_parameter(model,\n",
    "                    Parameters,\n",
    "                    param,\n",
    "                    parameter_list,\n",
    "                    X_train          = X_train,              # Defined above\n",
    "                    y_train          = y_train,\n",
    "                    verbose          = True,\n",
    "                    show_mae         = True,\n",
    "                    show_std         = False,\n",
    "                    n_iter_no_change = None,\n",
    "                    delta            = 0.001,\n",
    "                    n_jobs           = -1,\n",
    "                    n_repeats        = 5\n",
    "                   ):\n",
    "    \n",
    "\n",
    "    start = time.time()\n",
    "    Parameters = Parameters.copy()  # Avoid modifying the original dictionary\n",
    "    \n",
    "    cv_maes, std_cvs, train_maes = [], [], []\n",
    "    no_improve_count = 0\n",
    "    best_mae = float('inf')\n",
    "    \n",
    "    # Run over each value in parameter_list\n",
    "    for p in tqdm(parameter_list, desc=f\"Sweeping {param}\"):\n",
    "        Parameters[param] = p\n",
    "        P_temp = Parameters.copy()\n",
    "        P_temp.pop('MAE_found', None)  # Just in case\n",
    "        \n",
    "        cv_mae, std_cv, train_mae = run_model(\n",
    "            model=model,\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            n_repeats=n_repeats,\n",
    "            n_jobs=n_jobs,\n",
    "            **P_temp\n",
    "        )\n",
    "        cv_maes.append(cv_mae)\n",
    "        std_cvs.append(std_cv)\n",
    "        train_maes.append(train_mae)\n",
    "        \n",
    "        if cv_mae < best_mae - delta:\n",
    "            best_mae = cv_mae\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "        \n",
    "        if n_iter_no_change is not None and no_improve_count >= n_iter_no_change:\n",
    "            print(f\"Early stopping: No improvement after {n_iter_no_change} iterations.\")\n",
    "            break\n",
    "\n",
    "    # Identify best parameter\n",
    "    min_cv_mae = min(cv_maes)\n",
    "    min_index = cv_maes.index(min_cv_mae)\n",
    "    best_param = parameter_list[min_index]\n",
    "    Parameters[param] = best_param\n",
    "    Parameters['MAE_found'] = min_cv_mae\n",
    "\n",
    "    # ---------- Plotting section ----------\n",
    "    if verbose:\n",
    "        partial_param_list = parameter_list[:len(cv_maes)]\n",
    "\n",
    "        is_boolean = all(isinstance(val, bool) for val in partial_param_list)\n",
    "        if is_boolean:\n",
    "            x_vals = list(range(len(partial_param_list)))\n",
    "            x_labels = [str(val) for val in partial_param_list]\n",
    "        else:\n",
    "            x_vals = partial_param_list\n",
    "            x_labels = partial_param_list\n",
    "\n",
    "        error_name = 'MAE'\n",
    "\n",
    "        # Create appropriate number of subplots\n",
    "        if show_std:\n",
    "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\n",
    "        else:\n",
    "            fig, ax1 = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "        ax1.set_title(f\"{error_name} vs {param}\")\n",
    "        if show_mae:\n",
    "            ax1.yaxis.set_major_formatter(mticker.FuncFormatter(dollar_format))\n",
    "\n",
    "        ax1.plot(x_vals,\n",
    "                 cv_maes,\n",
    "                 marker='.', label=\"CV MAE\", color='blue')\n",
    "        ax1.plot(x_vals,\n",
    "                 train_maes,\n",
    "                 marker='.', label=\"Train MAE\", color='green')\n",
    "        ax1.scatter([x_vals[min_index]],\n",
    "                    [min_cv_mae],\n",
    "                    marker='x', label=\"Best CV MAE\", color='red')\n",
    "\n",
    "        ax1.set_ylabel(error_name)\n",
    "        ax1.legend()\n",
    "        ax1.grid()\n",
    "\n",
    "        # Optional Std Dev Plot\n",
    "        if show_std:\n",
    "            ax2.set_title(f\"CV Standard Deviation vs {param}\")\n",
    "            ax2.plot(x_vals, std_cvs, marker='.', label=\"CV MAE Std\", color='blue')\n",
    "            ax2.set_xlabel(param)\n",
    "            ax2.set_ylabel(\"Standard Deviation\")\n",
    "            ax2.legend()\n",
    "            ax2.grid(alpha=0.5)\n",
    "\n",
    "            if is_boolean:\n",
    "                ax2.set_xticks(x_vals)\n",
    "                ax2.set_xticklabels(x_labels)\n",
    "        else:\n",
    "            ax1.set_xlabel(param)\n",
    "            if is_boolean:\n",
    "                ax1.set_xticks(x_vals)\n",
    "                ax1.set_xticklabels(x_labels)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        end = time.time()\n",
    "        print(\"Execution Time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end - start)))\n",
    "\n",
    "    return Parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Iterative Parameter Sweeping and Visualization with `sweep_parameter(...)`\n",
    "\n",
    "In this problem, you’ll tune five key hyperparameters of `GradientBoostingRegressor` by manually sweeping their values and visualizing the results using **Mean Absolute Error (MAE)** as your evaluation metric.\n",
    "\n",
    "> **Important:** `GradientBoostingRegressor` runs on the CPU (not the GPU), even in Colab. Runtime depends heavily on:\n",
    ">\n",
    "> * Number of parameter values per sweep\n",
    "> * Number of cross-validation repeats\n",
    "> * `n_jobs` (parallel workers)\n",
    "\n",
    "Be strategic about computational cost.\n",
    "\n",
    "### Recommended Workflow\n",
    "\n",
    "**Phase A — Exploration (Fast Mode)**\n",
    "\n",
    "* Use `n_repeats = 1` or `2`\n",
    "* Limit each parameter sweep to **≤ 20 values**\n",
    "* Use coarse step sizes\n",
    "* Goal: identify promising regions quickly\n",
    "\n",
    "**Phase B — Refinement (Standard Mode)**\n",
    "\n",
    "* Use `n_repeats = 3`\n",
    "* Narrow ranges around best values\n",
    "* Use smaller step sizes\n",
    "* Limit sweeps to **≤ 15 values**\n",
    "\n",
    "**Phase C — Final Confirmation (Full Mode)**\n",
    "\n",
    "* Use `n_repeats = 5`\n",
    "* Only evaluate the final narrow ranges\n",
    "* Do not re-run large coarse sweeps at full repeats\n",
    "\n",
    "> If a single sweep is taking more than ~5–10 minutes, reduce:\n",
    ">\n",
    "> * number of parameter values\n",
    "> * `n_repeats`\n",
    "> * or set `n_jobs=2` or `4` instead of `-1`\n",
    "\n",
    "\n",
    "\n",
    "### Goal\n",
    "\n",
    "Your objective is to apply a **coarse-to-fine tuning strategy**: begin with broad parameter ranges and coarse steps, then narrow the range and increase granularity as you approach a good model.\n",
    "\n",
    "For example, when sweeping `n_estimators`, you might proceed as follows:\n",
    "\n",
    "**Exploration:**\n",
    "\n",
    "```python\n",
    "range(200, 1201, 200)  # 6 values\n",
    "```\n",
    "\n",
    "**Refinement:**\n",
    "\n",
    "```python\n",
    "range(800, 1201, 50)   # 8 values\n",
    "```\n",
    "\n",
    "**Final confirmation:**\n",
    "\n",
    "```python\n",
    "range(900, 1001, 10)   # 11 values\n",
    "```\n",
    "\n",
    "Avoid sweeping hundreds of values — this will dramatically increase runtime.\n",
    "\n",
    "\n",
    "### Step-by-Step Sweeping Procedure\n",
    "\n",
    "#### Tip on Repeats\n",
    "\n",
    "* Use **1–2 repeats** for early sweeps\n",
    "* Increase to **3 repeats** for refinement\n",
    "* Use **5 repeats only for final confirmation**\n",
    "\n",
    "Do not run large coarse sweeps with 5 repeats.\n",
    "\n",
    "\n",
    "#### 1. Sweep `n_estimators` (*integer values*)\n",
    "\n",
    "* Start with a **coarse range** (e.g., 200–1200 step 200)\n",
    "* Use `n_repeats = 1` or `2`\n",
    "* Plot results and update `Parameters_GB`\n",
    "* Narrow the range around the best region\n",
    "\n",
    "\n",
    "#### 2. Sweep `max_depth` (*integer values*)\n",
    "\n",
    "* Try small integers (e.g., 2–6 initially)\n",
    "* Use step size of 1 or 2\n",
    "* Avoid sweeping large depth ranges unnecessarily\n",
    "* Update `Parameters_GB`\n",
    "\n",
    "\n",
    "\n",
    "#### 3. Sweep `max_features` (*integer values*)\n",
    "\n",
    "* Check `X_train.shape` for the feature count\n",
    "* Use a reasonable subset (e.g., increments of 5 or 10)\n",
    "* Avoid sweeping from 1 to full feature count unless justified\n",
    "\n",
    "\n",
    "\n",
    "#### 4. Sweep `min_samples_split`\n",
    "\n",
    "* Try small integer ranges (e.g., 2–15 step 3–5)\n",
    "* Avoid large ranges\n",
    "\n",
    "\n",
    "\n",
    "#### 5. Sweep `min_samples_leaf`\n",
    "\n",
    "* Try small integers (e.g., 1–10 step 2–3)\n",
    "* Refine only if needed\n",
    "\n",
    "\n",
    "\n",
    "### Repeat with Finer Granularity\n",
    "\n",
    "After the first pass:\n",
    "\n",
    "* Narrow each parameter range around the best region\n",
    "* Use smaller step sizes\n",
    "* Reduce the number of candidate values\n",
    "* Increase `n_repeats` gradually\n",
    "\n",
    "You do **not** need to resweep every parameter each time.\n",
    "\n",
    "Eventually aim for:\n",
    "\n",
    "* `n_estimators`: step size of 10 (or 5 if runtime allows)\n",
    "* Others: step size of 1 or 2\n",
    "\n",
    "But only in narrow ranges.\n",
    "\n",
    "\n",
    "\n",
    "### Ensure Robustness\n",
    "\n",
    "Your final configuration must:\n",
    "\n",
    "* Use **at least 5 repeats**\n",
    "* Be based on narrow, well-justified ranges\n",
    "* Reflect clear CV MAE improvement\n",
    "\n",
    "This final configuration will form the foundation for Problem 2.\n",
    "\n",
    "\n",
    "\n",
    "### Final Reporting\n",
    "\n",
    "After completing 2–3 rounds of parameter sweeping:\n",
    "\n",
    "* Report final tuned values stored in `Parameters_GB`\n",
    "* Display final **CV MAE** clearly (in dollars)\n",
    "* Respond to the graded question\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Default_Parameters_GB = {\n",
    "    'n_estimators'            : 100,             # The number of boosting stages to be run. More estimators can improve performance but increase training time.\n",
    "    'max_depth'               : 3,               # Maximum depth of individual trees. Controls model complexity.\n",
    "    'max_features'            : None,            # Number of features to consider when looking for best split. Can help reduce overfitting.\n",
    "    'min_samples_split'       : 2,               # Defines the minimum number of samples required to split an internal node.\n",
    "    'min_samples_leaf'        : 1,               # Specifies the minimum number of samples that must be present in a leaf node. \n",
    "    'random_state'            : random_seed,     # Controls randomness of boosting. Useful for reproducibility.\n",
    "    'MAE_found'               : float('inf')     # NOT a model parameter, but will record the MAE found for the current parameter choices\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the default dictionary\n",
    "\n",
    "Params_GB = Default_Parameters_GB.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here, add as many cells as you need\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 Graded Answer\n",
    "\n",
    "Set `a1` to the CV MAE score of your best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Your answer here\n",
    "\n",
    "a1 = 0                     # replace 0 with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1 = $0.00\n"
     ]
    }
   ],
   "source": [
    "# DO NOT change this cell in any way\n",
    "\n",
    "print(f'a1 = ${a1:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Use `RandomizedSearchCV` to Explore Alternatives\n",
    "\n",
    "In this problem, you will use **randomized search** to explore the hyperparameter space more broadly than in Problem 1.\n",
    "\n",
    "Rather than sweeping one parameter at a time, `RandomizedSearchCV` samples random combinations of parameters from specified distributions. This allows you to explore interactions between parameters more efficiently than manual sweeps.\n",
    "\n",
    "Use the final parameter ranges you identified in Problem 1 — or slightly narrower ranges centered around your best values.\n",
    "\n",
    "\n",
    "### Setup Requirements\n",
    "\n",
    "* Define parameter distributions using `randint(lb, ub)` for **all** five parameters.\n",
    "\n",
    "  * Remember that `randint(a, b)` samples integers from `a` to `b - 1`.\n",
    "  * This gives you a granularity of 1 automatically.\n",
    "* Use `RepeatedKFold` for cross-validation so that your scoring is consistent with Problem 1.\n",
    "* Use `scoring='neg_mean_absolute_error'`.\n",
    "\n",
    "\n",
    "### Runtime Strategy (Very Important)\n",
    "\n",
    "Randomized search can become expensive quickly. Be strategic.\n",
    "\n",
    "Start conservatively:\n",
    "\n",
    "```python\n",
    "n_iter = 30\n",
    "n_repeats = 2\n",
    "```\n",
    "\n",
    "If runtime is comfortable (e.g., under ~15–20 minutes total), you may increase gradually:\n",
    "\n",
    "* Try `n_iter = 60`\n",
    "* Or increase to `n_repeats = 3`\n",
    "\n",
    "Only increase these values if your notebook is running smoothly and not approaching time or memory limits.\n",
    "\n",
    "> ⚠️ Do not immediately set `n_iter=100` with 5 repeats. That can result in thousands of model fits and long runtimes.\n",
    "\n",
    "Your goal is not to exhaust the search space — it is to intelligently explore it.\n",
    "\n",
    "\n",
    "\n",
    "### What to Report\n",
    "\n",
    "After fitting `RandomizedSearchCV`:\n",
    "\n",
    "* Print the best parameter combination (`best_params_`)\n",
    "* Report the corresponding **CV MAE**\n",
    "* Set `a2` equal to the best CV MAE found\n",
    "\n",
    "\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "It is entirely possible that randomized search does **not** outperform your manually tuned model from Problem 1.\n",
    "Randomized search is most valuable when:\n",
    "\n",
    "* The search space is large\n",
    "* Parameter interactions matter\n",
    "* Manual sweeping becomes inefficient\n",
    "\n",
    "Its purpose here is to broaden exploration and compare strategies — not to guarantee improvement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here -- Add as many code cells as necessary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 Graded Answer\n",
    "\n",
    "Set `a2` to the best CV MAE score found using randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Your answer here\n",
    "\n",
    "a2 = 0                     # replace 0 with your answer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2 = $0.00\n"
     ]
    }
   ],
   "source": [
    "# DO NOT change this cell in any way\n",
    "\n",
    "print(f'a2 = ${a2:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Evaluate and Select Your Final Model\n",
    "\n",
    "At this point, you may have **two candidate models**:\n",
    "\n",
    "1. The model obtained from **Problem 1** (manual parameter sweeps)\n",
    "2. The model obtained from **Problem 2** (`RandomizedSearchCV`)\n",
    "\n",
    "Your task is to select **one final model** and evaluate it on the **held-out test set**.\n",
    "\n",
    "\n",
    "### Step 1: Choose the Best Model (Using CV Only)\n",
    "\n",
    "Base your decision primarily on:\n",
    "\n",
    "* **Mean CV MAE**\n",
    "* Stability of CV results\n",
    "* Gap between training MAE and CV MAE (sign of overfitting)\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "* Prefer the model with the **lower CV MAE**\n",
    "* If CV MAE values are very close, prefer the model with:\n",
    "\n",
    "  * Smaller training–CV gap\n",
    "  * More stable behavior across parameter ranges\n",
    "* If the two models are indistinguishable within reasonable CV variability, either choice is acceptable.\n",
    "\n",
    "> Do **not** use the test set to decide between models.\n",
    "\n",
    "\n",
    "### Step 2: Train the Final Model and Evaluate on the Test Set\n",
    "\n",
    "Once you have selected your final configuration:\n",
    "\n",
    "* Instantiate a new `GradientBoostingRegressor` with the chosen parameters\n",
    "* Fit it on the **full training set**\n",
    "* Generate predictions on the **test set**\n",
    "* Compute the **test MAE**\n",
    "\n",
    "This test MAE is your final out-of-sample performance estimate.\n",
    "\n",
    "> **Important:** You may not use `run_model` here. That function is designed for cross-validation during tuning.\n",
    "> As in Homework 06, you must explicitly fit the final model and evaluate it on `X_test` and `y_test`.\n",
    "\n",
    "\n",
    "### What to Report\n",
    "\n",
    "* `a3a`: Which model you selected\n",
    "\n",
    "  * `1` = Problem 1 model\n",
    "  * `2` = Problem 2 model\n",
    "  * `3` = Essentially the same model\n",
    "* `a3b`: The final **test MAE** (in dollars)\n",
    "\n",
    "\n",
    "### Conceptual Reminder\n",
    "\n",
    "Cross-validation is used for **model selection**.\n",
    "The test set is used **once**, at the end, to estimate generalization performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 Graded Answers\n",
    "\n",
    "Set `a3a` to the number of the best model found:\n",
    "- 1 = Problem 1 model\n",
    "- 2 = Problem 2 model\n",
    "- 3 = Essentially the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Your answer here\n",
    "\n",
    "a3a = 0                     # replace 0 with one of 1, 2, or 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3a = 0\n"
     ]
    }
   ],
   "source": [
    "# DO NOT change this cell in any way\n",
    "\n",
    "print(f'a3a = {a3a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `a3b` to the test MAE of your best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Your answer here\n",
    "\n",
    "a3b = 0                     # replace 0 with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3b = $0.00\n"
     ]
    }
   ],
   "source": [
    "# DO NOT change this cell in any way\n",
    "\n",
    "print(f'a3b = ${a3b:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1: Which `GradientBoostingRegressor` parameters are most important?\n",
    "\n",
    "We will focus on the top **four** parameters in this list for `GradientBoostingRegressor`. \n",
    "\n",
    "---\n",
    "\n",
    "**Most Important Parameters**\n",
    "\n",
    "1. **learning_rate** (default: **0.1**)  \n",
    "   *Controls the contribution of each individual tree. A lower learning rate generally requires more trees but can lead to improved generalization.*\n",
    "\n",
    "2. **n_estimators** (default: **100**)  \n",
    "   *Specifies the number of boosting stages (i.e., the number of trees in the ensemble). More estimators can improve performance but also increase the risk of overfitting if not tuned properly.*\n",
    "\n",
    "3. **max_depth** (default: **3**)  \n",
    "   *Limits the depth of the individual regression trees. Restricting the depth helps control overfitting and reduces the complexity of each base learner.*\n",
    "\n",
    "4. **max_features** (default: **None**)  \n",
    "   *Controls the number of features to consider when looking for the best split. Adjusting this can influence the bias-variance trade-off of the model.*\n",
    "\n",
    "5. **min_samples_split** (default: **2**)  \n",
    "   *Defines the minimum number of samples required to split an internal node. This parameter controls the growth of each tree and can prevent overly specific splits.*\n",
    "\n",
    "6. **min_samples_leaf** (default: **1**)  \n",
    "   *Specifies the minimum number of samples that must be present in a leaf node. This helps in ensuring that trees do not become too tailored to the training data.*\n",
    "\n",
    "---\n",
    "\n",
    "**Less Important Parameters**\n",
    "\n",
    "7. **max_leaf_nodes** (default: **None**)  \n",
    "    *An optional parameter that sets a maximum number of leaf nodes for each tree. This can provide an additional way to control the complexity of the model.*\n",
    "\n",
    "8. **subsample** (default: **1.0**)  \n",
    "   *Determines the fraction of samples used for fitting each individual tree. Values less than 1.0 introduce randomness into the boosting process, which can help reduce overfitting.*\n",
    "\n",
    "9. **loss** (default: **'squared_error'**)  \n",
    "   *Determines the loss function to be optimized during training. Different loss functions can be used depending on the specific characteristics of the regression problem.*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Appendix 2: Tuning and Selecting Complex Models\n",
    "\n",
    "This appendix offers practical guidance for tuning complex models like Gradient Boosting and for interpreting validation results to choose the best-performing configuration. It combines strategy, visualization, and decision-making heuristics into one workflow.\n",
    "\n",
    "\n",
    "\n",
    "### 1. Using `sweep_parameter` for Single-Parameter Exploration\n",
    "\n",
    "The function `sweep_parameter` automates parameter tuning by iterating over a range of values (e.g., `n_estimators`) and tracking performance across:\n",
    "\n",
    "* **Training MAE**: Fit to training data\n",
    "* **Cross-Validation (CV) MAE**: Generalization estimate across folds (and repeats)\n",
    "\n",
    "**How to Interpret the Plots:**\n",
    "\n",
    "* **Training vs. CV MAE**: A growing gap often signals overfitting; high values for both may indicate underfitting.\n",
    "* **CV MAE Curve**: Choose values near the minimum (valley), ideally where the curve flattens.\n",
    "* **Watch Plot Scales**: A flat-looking curve may conceal meaningful differences if the y-axis scale is tight. Always consider actual values, not just shapes.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Tuning Strategy: Coarse-to-Fine\n",
    "\n",
    "* Start with **broad ranges and coarse steps** (e.g., 100–1000 by 100).\n",
    "* Once you find a promising region, **narrow the range and reduce step size** (e.g., 500–1000 by 25 or 10).\n",
    "\n",
    "\n",
    "### 3. Using Repeated Cross-Validation Effectively\n",
    "\n",
    "**Why Repeat?**\n",
    "Repeated CV provides more stable estimates by averaging results across multiple random folds, reducing variance due to data splits.\n",
    "\n",
    "**How Many Repeats?**\n",
    "\n",
    "* **Early (Broad Sweeps):** 1–2 repeats for speed\n",
    "* **Fine Tuning:** 5–10 repeats for stability and confidence in final selection\n",
    "\n",
    "**Trade-Offs:**\n",
    "\n",
    "* More repeats increase reliability, but also computation time. Scale up only after narrowing your search.\n",
    "\n",
    "\n",
    "### 4. Model Selection: Key Indicators\n",
    "\n",
    "When comparing models or parameter settings:\n",
    "\n",
    "* **Minimize Mean CV MAE**: This is your primary signal of generalization performance during model tuning.\n",
    "\n",
    "* **Look for Stability**: Favor flatter regions (plateaus) near the minimum of the CV MAE curve, rather than sharp dips that may reflect overfitting or noise.\n",
    "\n",
    "* **Avoid Overfitting**: If you examine the test MAE **after tuning,** be cautious of a *growing gap* between CV MAE and test MAE.\n",
    "\n",
    "  * This can suggest the model is fitting cross-validation folds too tightly.\n",
    "  * However, do **not** use test MAE to guide parameter choices — reserve it as a **final check** only.\n",
    "\n",
    "* **[Optional] Prioritize Consistency**: A lower standard deviation of CV scores indicates more stable performance across different splits.\n",
    "\n",
    "* **Interpret Plots Carefully**: Always consider the **scale** of the y-axis when comparing curves. Flat-looking trends might conceal meaningful differences if the axis range is small.\n",
    "\n",
    "\n",
    "### 5. Workflow Tips for Efficient Tuning\n",
    "\n",
    "* **Visualize Everything**: Always plot training/CV/test MAE of CV scores for insight.\n",
    "* **Track Experiments**: Use a dictionary (or a list of dictionaries) to record parameter settings and results.\n",
    "* **Scale Up Thoughtfully**: Start simple and add complexity (more repeats, finer search) only when needed.\n",
    "* **Use `GridSearchCV` and `RandomizedSearchCV` Judiciously**:\n",
    "\n",
    "  * Start with random search to identify promising regions, then use grid search for final tuning.\n",
    "  * Be aware that both methods may take significant time—especially without a progress bar.\n",
    "* **Enable Parallelism with `n_jobs=-1`**: This will use all available CPU cores.\n",
    "\n",
    "  * If you see warnings (e.g., “a worker stopped”), you may be running out of memory. Reduce `n_jobs` (e.g., to 4) if needed.\n",
    "\n",
    "\n",
    "\n",
    "By combining thoughtful parameter sweeps, smart use of repeated CV, and careful reading of validation curves and variance, you’ll build models that not only perform well but generalize reliably.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 3: Randomized Search vs. Grid Search for Gradient Boosting\n",
    "\n",
    "This appendix compares two strategies for hyperparameter tuning—**randomized search** and **grid search**—using `GradientBoostingRegressor` as the working example. \n",
    "\n",
    "### 1. Problem Setup\n",
    "\n",
    "Suppose you are training a `GradientBoostingRegressor` and want to optimize its predictive performance by tuning the key hyperparameters above.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model = GradientBoostingRegressor(random_state=random_seed)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 2. Grid Search\n",
    "\n",
    "**Grid search** evaluates **all** combinations of parameter values in a predefined grid.\n",
    "\n",
    "#### Define Parameter Grid\n",
    "\n",
    "```python\n",
    "param_grid = {\n",
    "\n",
    "    'n_estimators'      : range(1000,1501,100),\n",
    "    'max_depth'         : range(5,51,5),\n",
    "    'max_features'      : [3, 4, 5],\n",
    "    'min_samples_split' : [2,4,6,8],             \n",
    "    'min_samples_leaf'  : [1,2,3],   \n",
    "}\n",
    "```\n",
    "\n",
    "#### Run `GridSearchCV`\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=GradientBoostingRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=5, random_state=42), \n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "```\n",
    "\n",
    "* **Pros**: Exhaustive—guarantees best settings *within* your grid\n",
    "* **Cons**: Combinatorial explosion—3×3×3×2 = 54 fits! Time and resources grow rapidly\n",
    "\n",
    "\n",
    "\n",
    "### 3. Randomized Search\n",
    "\n",
    "**Randomized search** samples a fixed number of combinations from **distributions** over the parameter space.\n",
    "\n",
    "#### Define Distributions\n",
    "\n",
    "This is a significant difference with grid search: you must specify a random-number generator instead of\n",
    "giving a list of explicit values. \n",
    "\n",
    "```python\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators'      : randint(100, 500),        # integers from 100 to 499\n",
    "    'max_depth'         : randint(3, 8),            # integers from 3 to 7\n",
    "    'max_features'      : randint(2, 15),            \n",
    "    'min_samples_split' : randint(2, 10),             \n",
    "    'min_samples_leaf'  : randint(1, 8),             \n",
    "}\n",
    "```\n",
    "\n",
    "#### Run `RandomizedSearchCV`\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rand = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,                 # try 20 random combinations\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_seed), \n",
    "    scoring='neg_mean_absolute_error',\n",
    "    random_state=random_seed,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rand.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", rand.best_params_)\n",
    "```\n",
    "\n",
    "* **Pros**: Efficient—fixed number of combinations regardless of parameter count\n",
    "* **Cons**: May miss the absolute best—but often finds near-optimal solutions much faster\n",
    "\n",
    "\n",
    "\n",
    "### 4. When to Use Which?\n",
    "\n",
    "| Scenario          | Use Grid Search                 | Use Randomized Search               |\n",
    "| ----------------- | ------------------------------- | ----------------------------------- |\n",
    "| Search space size | Small, well-defined             | Large or continuous                 |\n",
    "| Compute budget    | High                            | Moderate or limited                 |\n",
    "| Goal              | Exhaustive search within a grid | Fast discovery of promising regions |\n",
    "| Parameter types   | Discrete                        | Continuous or mixed                 |\n",
    "| Tuning stage      | Final fine-tuning               | Early-stage exploration             |\n",
    "\n",
    "\n",
    "\n",
    "### 5. Best Practices\n",
    "\n",
    "1. **Start with Randomized Search**\n",
    "\n",
    "   * Identify promising parameter ranges quickly and cheaply.\n",
    "\n",
    "2. **Refine with Grid Search**\n",
    "\n",
    "   * Use a tighter grid centered around the best values from the randomized search.\n",
    "\n",
    "3. **Use Log Scale for Learning Rates**\n",
    "\n",
    "   * For scale-sensitive values like `learning_rate`, sample from log-distributions (e.g., `loguniform(1e-3, 1e0)`).\n",
    "\n",
    "4. **Validate on a Held-Out Test Set**\n",
    "\n",
    "   * After tuning, always check your model’s performance on a separate test set to assess generalization.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
